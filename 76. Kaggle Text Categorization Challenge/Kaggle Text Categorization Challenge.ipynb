{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for NLP\n",
    "\n",
    "In the earlier section, we used classical machine learning techniques to build our text classifiers. In this chapter, we will replace those with deep learning techniques: Recurrent Neural Networks. \n",
    "\n",
    "In particular, we will use a relatively simple LSTM model. If this is new to you, keep reading - if not, please feel free to skip ahead! \n",
    "\n",
    "I begin by touching upon the overhyped terms as 'deep' in Deep Learning and 'neural' in Deep Neural Networks. I do a quick detour to why I use PyTorch and compare it to Tensorflow and Keras - the other popular Deep Learning frameworks.   \n",
    "\n",
    "I will build the simplest possible architecture for demonstration here. I assume a general familiarity with RNNs and don’t introduce the same again.  \n",
    "\n",
    "In this section, we answer the following questions: \n",
    "- What is Deep Learning? How does it differ from what we have seen? \n",
    "- What are the key ideas in any deep learning model? \n",
    "- Why PyTorch?  \n",
    "- How to tokenize text and setting up dataloaders with `torchtext`?\n",
    "- What recurrent networks are, and how to use them for text classification? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Deep Learning? \n",
    "\n",
    "Deep learning is a subset of machine learning: a new take on learning from data that puts an emphasis on learning successive layers of increasingly meaningful representations. But what does 'deep' in Deep Learning mean? \n",
    "\n",
    "> The deep in deep learning isn’t a reference to any kind of deeper understanding achieved by the approach; rather, it stands for this idea of successive layers of representations. - F. Chollet, Lead Developer of Keras\n",
    "\n",
    "The _depth_ of the model is indicative of how many layers of such representations did we use. F Chollet suggested _layered representations learning_, _hierarchical representations learning_ as better names for these. Another name could have been _differentiable programming_. This term coined by Yann LeCun, reasons from that the common thing between our 'deep learning methods' are not more layers. Instead, that all these models learn via some form of differential calculus—most often stochastic gradient descent.\n",
    "\n",
    "## Differences with 'Modern' Machine Learning Methods\n",
    "The modern machine learning methods which we saw shot to mainstream mostly in the 1990s or after that. The binding factor among them was that they all use one layer of representations. For instance, the Decision Trees just create one set of rules and apply them. Even if you add ensemble approaches, the 'ensembling' is often shallow and only combines several ML models directly.\n",
    "\n",
    "Here is a better worded interpretation of these differences: \n",
    "\n",
    "> Modern deep learning often involves tens or even hundreds of successive layers of representations—and they’re all learned automatically from exposure to training data. Meanwhile, other approaches to machine learning tend to focus on learning only one or two layers of representations of the data; hence, they’re sometimes called shallow learning. - F Chollet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Deep Learning\n",
    "\n",
    "In a loosely worded manner, machine learning is about mapping inputs (such as image, or \"movie review\") to targets (such as the label cat or “positive”). The model does this by looking at (or training from) several pairs of input and targets. \n",
    "\n",
    "Deep Neural Networks do this input-to-target mapping using a long sequence of simple data transformations (layers). This sequence length is referred to as depth of the network. The entire sequence from input-to-target is referred to as _model_ which learns about the data. These data transformations are learned by repeated obvservation of examples.  let’s look at how this learning happens, concretely.\n",
    "\n",
    "As a necessary caveat here, the _neural_ in Neural Networks has nothing to do with human brain except serving as a bad metaphor. There are several articles written by lazy journalists who did not bother asking any actual Deep Learning engineer or researcher about this term. You can safely ignore _all of them_ \n",
    "\n",
    "## Puzzle Pieces\n",
    "\n",
    "We are looking at a particular sub-class of challenges where we want to learn an input-to-target mapping. This subclass is generally referred to as supervised machine learning. The word _supervised_ denoting that we have target(s) for each input. Unsupervised machine learning includes challenges like trying to cluster text, where we do not have a target. \n",
    "\n",
    "In order to do any supervised machine learning, we need the following in-place: \n",
    "\n",
    " 1. Input Data : Anything ranging from past stock performance to your vacation pictures \n",
    " 1. Target - Examples of the expected output:\n",
    " 1. A way to measure whether the algorithm is doing a good job — This is necessary in order to determine the distance between the algorithm’s current output and its expected output. \n",
    " \n",
    "The above components are universal to any supervised approach, machine learning or deep learning. Deep Learning in particular has it's own starcast of puzzle pieces: \n",
    "\n",
    "1. Model Itself  \n",
    "1. Loss Function\n",
    "1. Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since these actors are new to the scene, let's take a minute in understanding what they do: \n",
    "\n",
    "### Model\n",
    "\n",
    "Each model is comprised of several layers. Each layer is a data transformation. The transformation is captured using a bunch of numbers - called layer weights. This is not complete truth though, most layers often have an operation mathematical associated with it e.g. convolution or affine transform. A more precise perspective would be to say that a layer is **parameterized** by it's weights. Hence, we use the terms _layer parameters_ and _layer weights_ interchangeably. \n",
    "\n",
    "The state of all the layer weights together makes the model state captured in model weights. A model can have anywhere between a few thousand to few million parameters. \n",
    "\n",
    "Let's try to understand the notion of model **learning** in this context:\n",
    "\n",
    "Learning means finding values for the weights of all layers in a network, such that the network will correctly map example inputs to their associated targets. Note that this value set is for _all layers_ at one go. This nuance is important because changing weights of one layer can change the behaviour and predictions made by the entire model. \n",
    "\n",
    "### Loss Function\n",
    "One of the pieces to setup a Machine learning task is to assess how a model is doing. The simplest answer would be to measure a notional accuracy of the model. Accuracy has few flaw thoughs:\n",
    "- Accuracy is a proxy metric tied to validation data and not training data\n",
    "- Accuracy measures how correct we are. During training, we want to measure how far are model predictions from target. \n",
    "\n",
    "These differences mean we need a different function to meet our criteria above. This is fulfilled by the _loss function_ in context of Deep Learning. This is sometimes referred to as an _objective function_ as well.\n",
    "\n",
    "> The loss function takes the predictions of the network and the true target (what you wanted the network to output) and computes a distance score, capturing how well the network has done on this specific example. \n",
    "> - From Deep Learning in Python by F Chollet\n",
    " \n",
    "This distance measurement is called loss score or simply loss.\n",
    "\n",
    "### Optimizer\n",
    "\n",
    "This loss is automatically used as a feedback signal to adjust the way the algorithm works. This adjustment step is what we call learning.\n",
    "\n",
    "This automatic ajustment in model weights is peculiar to deep learning. Each adjustment or _update_ of weights is made in a direction that will lower the loss score for the current training pair (input, target). \n",
    "\n",
    "This adjustment is the job of the optimizer, which implements what’s called the Backpropagation algorithm: the central algorithm in deep learning. \n",
    "\n",
    "Optimizers and loss functions are common to all deep learning methods - even the cases where we don't have a (input, target) pair. All optimizers are based on differential calculus such as Stochastic Gradient Descent (SGD), Adam and so on. Hence, the term differentiable programming is a more precise name for Deep Learning in my mind. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it Together: Training Loop\n",
    "\n",
    "We now have a shared vocabulary. You have a notional understanding of what terms like layers, model weights, loss function, optimizer mean. But how do they work together? How do we train them on arbitrary data? We can train them to give us the ability to recognise cat picture to fraud reviews on Amazon. \n",
    "\n",
    "Here is the rough outline of steps that happen inside a training loop: \n",
    "\n",
    "- Initialize: \n",
    "    - The network/model weights are assigned random values, usually in (-1, 1) or (0, 1) \n",
    "    - Model is very far from the target. This is because it is simply executing a series of random transformations. \n",
    "    - Loss is very high \n",
    "- With every example the network processes:\n",
    "    - Weights are adjusted a little in the correct direction\n",
    "    - Loss score decreases\n",
    "    \n",
    "This is the training loop, which is repeated several times. Each pass over the entire training set is often referred to an _epoch_. Each training set suited for deep learning should typically have thousands of examples. The models are often trained for tens of epochs, or alternatively millions of iterations.\n",
    "\n",
    "In a training setup (model, optimizer, loop), the above loop updates weight values that minimize the loss function. A  trained network is the one with least possible loss score on the entire training and valid data. \n",
    "\n",
    "It’s a simple mechanism that, repeated often times, just works like magic. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle: Text Categorization Challenge\n",
    "\n",
    "In this particular section, we are going to visit the familiar task of text classification. We are going to use a different datset though. We are going to be solving the [Jigsaw Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge).\n",
    "\n",
    "# Getting the Data\n",
    "\n",
    "Note that you will need to do accept the terms and conditions of the competition and data usage in order to get this dataset.\n",
    "\n",
    "**Direct Download**: You can get the train and test data from the [data tab on challenge website](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data).  \n",
    "\n",
    "**Kaggle API**: You can use the official Kaggle API [(github link)](https://github.com/Kaggle/kaggle-api) to download the data\n",
    "\n",
    "In case of direct download and Kaggle API both, you have to split your train data into smaller train and validation splits for this notebook. \n",
    "\n",
    "You can create train and valid splits of train data using `sklearn.model_selection.train_test_split` utility. Alternatively, you can download directly from the accompanying ...\n",
    "\n",
    "**Github Repo**: I am uploading the exact splits that I am using to a repository associated with this codebase. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install -y pandas\n",
    "# !conda install -y numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\r\\nWhy the edits made under my use...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\r\\nMore\\r\\nI can't make any real suggestions...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\r\\nWhy the edits made under my use...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\r\\nMore\\r\\nI can't make any real suggestions...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = pd.read_csv(\"data/valid.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000eefc67a2c930f</td>\n",
       "      <td>Radial symmetry \\r\\n\\r\\nSeveral now extinct li...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000f35deef84dc4a</td>\n",
       "      <td>There's no need to apologize. A Wikipedia arti...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000ffab30195c5e1</td>\n",
       "      <td>Yes, because the mother of the child in the ca...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0010307a3a50a353</td>\n",
       "      <td>\"\\r\\nOk. But it will take a bit of work but I ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0010833a96e1f886</td>\n",
       "      <td>\"== A barnstar for you! ==\\r\\n\\r\\n  The Real L...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  000eefc67a2c930f  Radial symmetry \\r\\n\\r\\nSeveral now extinct li...      0   \n",
       "1  000f35deef84dc4a  There's no need to apologize. A Wikipedia arti...      0   \n",
       "2  000ffab30195c5e1  Yes, because the mother of the child in the ca...      0   \n",
       "3  0010307a3a50a353  \"\\r\\nOk. But it will take a bit of work but I ...      0   \n",
       "4  0010833a96e1f886  \"== A barnstar for you! ==\\r\\n\\r\\n  The Real L...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Target Dataset!\n",
    "\n",
    "The interesting thing about this dataset is that each comment can have multiples labels. For instance, a comment can be insult and be toxic. Or be obscene and have identity_hate elements in it. \n",
    "\n",
    "Hence, we are leveling up here by trying to predict not one label (e.g. positive or negative) but multiple labels at one go. For each label, we'd predict a value between 0 and 1 to indicate how likely it is to belong to that category. \n",
    "\n",
    "This is not a probability value in the Bayesian meaning of the word, but represents the same intent. \n",
    "\n",
    "Tip, I'd recommend trying out the models which we have seen earlier with this dataset. \n",
    "\n",
    "And re-implementing this code for our favourite IMDb dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>== From RfC == \\r\\n\\r\\n The title is fine as i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>\" \\r\\n\\r\\n == Sources == \\r\\n\\r\\n * Zawe Ashto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>:If you have a look back at the source, the in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>I don't anonymously edit articles at all.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text\n",
       "0  00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...\n",
       "1  0000247867823ef7  == From RfC == \\r\\n\\r\\n The title is fine as i...\n",
       "2  00013b17ad220c46  \" \\r\\n\\r\\n == Sources == \\r\\n\\r\\n * Zawe Ashto...\n",
       "3  00017563c3f7919a  :If you have a look back at the source, the in...\n",
       "4  00017695ad8997eb          I don't anonymously edit articles at all."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why PyTorch? \n",
    "\n",
    "PyTorch is a deep learning framework by Facebook, similar to Tensorflow by Google. \n",
    "\n",
    "Being backed by Google, thousands of dollars have been spent in Tensorflow's marketing, development and documentation. It also got to a stable 1.0 release almost a year ago, while PyTorch has only recently gotten to 0.4.1. This means, that it's usually easier to find a Tensorflow solutiton to your problem and you can copy paste code off Internet. \n",
    "\n",
    "On the other hand, PyTorch is programmer friendly. It is semantically similar to numpy+deep learning operations as one. This means I can use the Python debugging tools that I am already familiar with it. \n",
    "\n",
    "Pythonic: Tensorflow worked like a C program in the sense that the code was all written in one session, compiled and then executed. Thereby destroying it's Python flavour altogether. This has been solved by Tensorflow's Eager Execution feature release, which will soon be stable enough to use for most prototyping work. \n",
    "\n",
    "Trainig Loop Visualization: Till a while ago, Tensorflow had a good visualization tool called Tensorboard for understanding how your training and validation performance (and other characterstics) which was absent in PyTorch. For a long while now, tensorboardX makes Tensorboard easy to use with PyTorch.\n",
    "\n",
    "In summary, I use PyTorch because it is easier to debug, more Pythonic and more programmer friendly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch and torchtext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can install the latest version of Pytorch ([website](https://pytorch.org/)) via conda or pip for your target machine. I am running this code on a Windows laptop with a GPU. \n",
    "\n",
    "I installed `torch` using `conda install pytorch cuda92 -c pytorch`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install -y pytorch cuda92 -c pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For installing `torchtext`, I recommend using pip directly from their Github repository with the latest fixes instead of PyPi which is not frequently updated. Uncomment the install line when running this for the first time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade git+https://github.com/pytorch/text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchtext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are running this code on a machine with GPU, leave the `use_gpu` flag set to True, else set it to False. \n",
    "\n",
    "If you set `use_gpu=True` on a machine, we can check whether the GPU is accessible to PyTorch or not using the `torch.cuda.is_available()` utility. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = True\n",
    "if use_gpu:\n",
    "    assert torch.cuda.is_available(), 'You either do not have a GPU or is not accessible to PyTorch'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many GPU devices are available to PyTorch on this machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loaders with torchtext\n",
    "\n",
    "Writing good data loaders is the most tedious part in most deep learning applications. This step often combines the preprocessing, text cleaning, and vectorization tasks which we have seen earlier. \n",
    "\n",
    "Additionally, it wraps our static data objects into iterators or generators. This is incredibly helpful in processing data sizes much larger than GPU memory - which is quite often the case. This is done by splitting the data such that you can make `batches` of **batchsize** samples such that it fits your GPU memory.\n",
    "\n",
    "Batchsizes are often powers of 2, such as 32, 64, 512 and so on. This convention exists because it helps with vector operations on the instruction set level. Anecdotally, using a batchsize different from power of 2 has not helped or hurt my processing speed.\n",
    "\n",
    "### Conventions and Style\n",
    "The code, iterators and wrappers used below are from [Practical Torchtext](https://github.com/keitakurita/practical-torchtext/). It is a torchtext tutorial by Keita Kurita - one of the top 5 contributors to torchtext. \n",
    "\n",
    "The naming conventions and style are loosely inspired from the above work and fastai - a deep learning framework based on PyTorch itself.\n",
    "\n",
    "**Let's begin**  by setting up the required variable placeholders in place: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Field "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Field class determines how the data is preprocessed and converted into a numeric format. The Field class is a fundamental torchtext data structure and worth looking into. Field class models common text processing and sets them up for numericalisation (or vectorisation). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL = Field(sequential=False, use_vocab=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All fields, by default, expect a sequence of words to come in, and they expect to build a mapping from the words to integers later on. This mapping is called the vocab, and is effectively one hote encoding of the the tokens.  \n",
    "\n",
    "We saw that each label in our case is already an integer marked as 0 or 1. So, we will not one-hot this, we tell the Field class that is already one-hot encoded and non-sequential by setting, `use_vocab=False` and `sequential=False` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize = lambda x: x.split()\n",
    "TEXT = Field(sequential=True, tokenize=tokenize, lower=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here as few things happening, let's unpack it a bit: \n",
    "\n",
    "- `lower=True`: all input is converted to lowercase\n",
    "- `sequential=True`: if False, no tokenization is applied\n",
    "- tokenizer: we defined a custom tokenize function which simply splits the string on space. You should replace this with the spaCy tokenizer (set tokenize=\"spacy\") and see if that changes the loss curve or final model performance. \n",
    "\n",
    "**More about `Field`: **\n",
    "\n",
    "In addition to the keyword arguments mentioned above, the Field class also allows the user to specify special tokens (the unk_token for out-of-vocabulary _unknown_ words, the pad_token for padding, the eos_token for the end of a sentence, and an optional init_token for the start of the sentence). \n",
    "\n",
    "The preprocessing and postprocessing parameters accept any `torchtext.data.Pipeline`s. Preprocessing is applied after tokenizing but before numericalizing. Postprocessing is applied after numericalizing, but before converting them to a Tensor.\n",
    "\n",
    "The docstrings for the Field class are relatively well written, so if you need some advanced preprocessing you should probe them for more information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import TabularDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TabularDataset is the class that we use to read is csv, tsv or json files.  You can specify the type of file that you are reading 'csv', 'tsv' or 'json' directly in the API. It's a powerful and handy API.\n",
    "\n",
    "At first glance, you might this that class is a bit misplaced because a generic file I/O+processor API should be accessible directly in PyTorch and not in a package dedicated to text processing. Let's see why it is placed where it is. \n",
    "\n",
    "`TabularData` has an interesting `fields` parameter. For the CSV data format, fields is a list of tuples. Each tuple in turn is the column name and the torchtext variable we want to associate with it. The fields should be in the same order as eh columns in the CSV or TSV. \n",
    "\n",
    "We have only two defined fields here: TEXT and LABEL. So each column is tagged as either one. We can simply mark the column as `None` if we want to ignore it completely. This is how we are tagging our columns as inputs (TEXT) and targets (LABEL) for the model to learn. \n",
    "\n",
    "This tight coupling of the `fields` parameter with `TabularData` is why this is a part of torchtext and not PyTorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_datafields = [(\"id\", None), # we won't be needing the id, so we pass in None as the field\n",
    "                 (\"comment_text\", TEXT), (\"toxic\", LABEL),\n",
    "                 (\"severe_toxic\", LABEL), (\"threat\", LABEL),\n",
    "                 (\"obscene\", LABEL), (\"insult\", LABEL),\n",
    "                 (\"identity_hate\", LABEL)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This defines our list of inputs. I have done this manually here, but you could also do this with code by reading the column headers from our `train_df` and assigning them TEXT or LABEL accordingly. \n",
    "\n",
    "As a reminder, we will have to define another fields list for our test data because it has a different header. It has no LABEL fields. \n",
    "\n",
    "TabularDataset supports two APIs: `split` and `splits`. We will use the one with the extra s, `splits`. The splits API is simple:\n",
    "- path: is the prefix of filenames\n",
    "- train, validation: are filenames of the corresponding dataset\n",
    "- format: one 'csv', 'tsv' or 'json' as stated earlier. Set to 'csv' here\n",
    "- skip_header: set to True if your csv file has column titles in it as does ours\n",
    "- fields: we pass the list of fields we just setup above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn, vld = TabularDataset.splits(\n",
    "        path=\"data\", # the root directory where the data lies\n",
    "        train='train.csv', validation=\"valid.csv\",\n",
    "        format='csv',\n",
    "        skip_header=True, # if your csv header has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
    "        fields=tv_datafields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repeat the above for test data now. We drop the 'id' column again and set 'comment_text' to be our label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_datafields = [(\"id\", None), # we won't be needing the id, so we pass in None as the field\n",
    "                 (\"comment_text\", TEXT)\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pass the entire relative filepath directly in path, instead of using the path+test variable combination here. We used the path+train combination when setting up the `trn` and `vld` variables. \n",
    "\n",
    "As a note, these filenames are consistent with what Keita used in the torchtext tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = TabularDataset(\n",
    "        path=\"data/test.csv\", # the file path\n",
    "        format='csv',\n",
    "        skip_header=True, # if your csv header has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
    "        fields=tst_datafields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Dataset Objects\n",
    "\n",
    "Let's look at the dataset objects, trn and valid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torchtext.data.dataset.TabularDataset at 0x176c45f30f0>,\n",
       " <torchtext.data.dataset.TabularDataset at 0x176c45f3908>,\n",
       " <torchtext.data.dataset.TabularDataset at 0x176c45f32b0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn, vld, tst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are all objects from the same class. Our dataset objects can be indexed and iterated over like normal lists, so let’s see what the first element looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torchtext.data.example.Example at 0x176c45f3978>,\n",
       " <torchtext.data.example.Example at 0x176c45f3940>,\n",
       " <torchtext.data.example.Example at 0x176c45f37f0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn[0], vld[0], tst[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All our first elements are in turn, objects of the `example.Example` class. Each example stores each column as an attribute. But where did our text and labels go?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['comment_text', 'toxic', 'severe_toxic', 'threat', 'obscene', 'insult', 'identity_hate'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn[0].__dict__.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Example object bundles the attributes of a single data point together. Our text comments and the labels are now part of the dictionary which makes up each of these example objects. We found all of them by calling the `__dict__.keys()` on an `example.Example` object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['explanation', 'why', 'the', 'edits', 'made']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn[0].__dict__['comment_text'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text has already been tokenized for us, but has not yet been vectorized or numericalized. We will use one hot encoding for all the tokens which exist in our training corpus. This will convert our words into integers. \n",
    "\n",
    "We can do this by calling the `build_vocab` attribute of our TEXT field. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(trn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This statement processes the entire train data - in particular, the 'comment_text' field. The words are registered in the vocabulary. \n",
    "\n",
    "Torchtext has its own class called Vocab for handling the vocabulary. The `Vocab` class can also take options like `max_size` and `min_freq` that dictate how many words are in the vocabulary or how many times a word has to appear to be registered in the vocabulary. \n",
    "\n",
    "Words that are not included in the vocabulary will be converted into `<unk>`, a token meaning for “unknown”. Words which occur are too rare are also assigned the `<unk>` token for ease of processing. This can hurt or help the model performance, depending which and how many words do we lose to the `<unk>` token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.vocab.Vocab at 0x176c45f3400>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TEXT field now has a vocab attribute which is a specific instance of the Vocab class.  We can use this in turn to look up the attributes of the vocab object. For instance, we can find the frequency of any word in the training corpus. The `TEXT.vocab.freqs` object is actually an object of the type `collections.Counter`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "collections.Counter"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(TEXT.vocab.freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means it will support all function, including the most_common API to sort the words by frequency and find top k most frequently occuring words for us. Let's take a look at them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 78), ('to', 41), ('you', 33), ('of', 30), ('and', 26)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.freqs.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Vocab class holds a mapping from word to id in its stoi attribute and a reverse mapping in its itos attribute. Let's look at these attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, collections.defaultdict, 784, 784)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(TEXT.vocab.itos), type(TEXT.vocab.stoi), len(TEXT.vocab.itos), len(TEXT.vocab.stoi.keys()), "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**itos** or the integer to string mapping is a list of words. The index of each word in the list is it's integer mapping. For instance, the 7-indexed word would be 'and' because it's integer mapping is 7. \n",
    "    \n",
    "**stoi** or string to integer mapping is a dictionary of words. Each key is a word in the training corpus, with the value being an integer. For instance, the word 'and' might have an integer mapping which can be looked up in this dictionary in O(1) time.\n",
    "\n",
    "Note that this convention automatically handles the off-by-one problem cause by zero indexing in Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 'and')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.stoi['and'], TEXT.vocab.itos[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterators!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torchtext has renamed and extended the `DataLoader` objects from PyTorch and torchvision. In essence, it does the same three jobs:\n",
    "\n",
    "- Batching the data\n",
    "- Shuffling the data\n",
    "- Load the data in parallel using `multiprocessing` workers\n",
    "\n",
    "\n",
    "This batch loading of data enables us to process a dataset much larger than the GPU RAM. `Iterator`s extend and specialize the `DataLoader` for NLP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Iterator, BucketIterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BucketIterator\n",
    "\n",
    "BucketIterator automatically shuffles and buckets the input sequences into sequences of similar length.\n",
    "\n",
    "To enable batch processing, we need the input sequences in a batch to be of identical length. This is done by padding the smaller input sequences to the length of longest sequence in batch. Hence, we also saw the <pad> token in our vocabulary earlier. \n",
    "    \n",
    "```\n",
    "[ [3, 15, 2, 7], \n",
    "  [4, 1], \n",
    "  [5, 5, 6, 8, 1] ]\n",
    "```\n",
    "\n",
    "would need to be padded to become\n",
    "```\n",
    "[ [3, 15, 2, 7, 0],\n",
    "  [4, 1, 0, 0, 0],\n",
    "  [5, 5, 6, 8, 1] ]\n",
    "```\n",
    "\n",
    "Additonally, the padding opertation is most efficient when the sequences are of similar lengths. The `BucketIterator` does all this behind the scenes. This is what makes it an extremely powerful abstraction for text processing. \n",
    "\n",
    "We want to bucket based on the lengths of the __comment_text__ field, so we pass that in as a keyword argument. \n",
    "\n",
    "Let's go aheand and initialize the `Iterator`s for the train and validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, val_iter = BucketIterator.splits(\n",
    "        (trn, vld), # we pass in the datasets we want the iterator to draw data from\n",
    "        batch_sizes=(32, 32),\n",
    "        sort_key=lambda x: len(x.comment_text), # the BucketIterator needs to be told what function it should use to group the data.\n",
    "        sort_within_batch=False,\n",
    "        repeat=False # we pass repeat=False because we want to wrap this Iterator layer.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick glance at the parameters we passed to this function: \n",
    "\n",
    "- batch_size: We use small batch size of 32 for both train and valid. This is because I am using a GTX 1060 with only 3 GB of memory\n",
    "- sort_key: BucketIterator is told to use the number of tokens in the comment_text as the key to sort in any example\n",
    "- sort_within_batch argument: When set to True, sorts the data within each minibatch in decreasing order according to the sort_key\n",
    "- repeat: when set to True, it allows us to loop over and see a previously seen sample again. We set it to False here because we are repeating using an abstraction that we will write in a minute. \n",
    "\n",
    "In the meanwhile, let's take quick minute to explore the new variable which we just made: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.data.iterator.BucketIterator at 0x176c45f30b8>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[torchtext.data.batch.Batch of size 25]\n",
       "\t[.comment_text]:[torch.LongTensor of size 494x25]\n",
       "\t[.toxic]:[torch.LongTensor of size 25]\n",
       "\t[.severe_toxic]:[torch.LongTensor of size 25]\n",
       "\t[.threat]:[torch.LongTensor of size 25]\n",
       "\t[.obscene]:[torch.LongTensor of size 25]\n",
       "\t[.insult]:[torch.LongTensor of size 25]\n",
       "\t[.identity_hate]:[torch.LongTensor of size 25]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(train_iter.__iter__())\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, all that each batch has are torch tensors of exactly same size (size is length of vector of vector of vector of vector of vector of vector of vector here). These tensors have not been move to GPU yet, but that's fine.\n",
    "\n",
    "`batch` is acutally a wrapper over the alreday familiar example object which we have seen. It bundles all the attributes relates to the batch in one variable dict: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['batch_size', 'dataset', 'fields', 'comment_text', 'toxic', 'severe_toxic', 'threat', 'obscene', 'insult', 'identity_hate'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.__dict__.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our above understanding is correct, and we know how Python's object passing works - the dataset attribute of the batch variable should point to the variable `trn` of `torchtext.data.TabularData` type. Let's check for this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torchtext.data.dataset.TabularDataset at 0x176c45f30f0>,\n",
       " <torchtext.data.dataset.TabularDataset at 0x176c45f30f0>,\n",
       " True)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.__dict__['dataset'], trn, batch.__dict__['dataset']==trn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aha! We got this right. \n",
    "\n",
    "For the test iterator, since we don not need shuffling - we will use the plain torchtext `Iterator`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_iter = Iterator(tst, batch_size=64, sort=False, sort_within_batch=False, repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[torchtext.data.batch.Batch of size 33]\n",
       "\t[.comment_text]:[torch.LongTensor of size 158x33]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(test_iter.__iter__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sequence length 33 here is different from the input's 25. That is fine. We see that this is also a torch tensor now. \n",
    "\n",
    "Next, let's write a wrapper over the batch objects:\n",
    "\n",
    "## BatchWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the problem with `batch` objects? **\n",
    "\n",
    "Our Batch iterator returns a custom datatype called `torchtext.data.Batch`. This has a similar API to the `example.Example`. This returns with a batch of data from each field as attributes. This custom datatype makes code reuse difficult since each time the column names change, we need to modify the code. \n",
    "This also makes torchtext hard to use with other libraries like torchsample and fastai. \n",
    "\n",
    "**How do we solve for this?**\n",
    "We will convert the batch to a tuple in the form (x, y). `x` is the input to model and `y` is the target. Or more convetionally, x is the indpendent variable while y is the dependent variable. One way to think about this is that the model will learn the function mapping from x to y. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchWrapper:\n",
    "    def __init__(self, dl, x_var, y_vars):\n",
    "        self.dl, self.x_var, self.y_vars = dl, x_var, y_vars # we pass in the list of attributes for x and y\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for batch in self.dl:\n",
    "            x = getattr(batch, self.x_var) # we assume only one input in this wrapper\n",
    "            \n",
    "            if self.y_vars is not None: # we will concatenate y into a single tensor\n",
    "                y = torch.cat([getattr(batch, feat).unsqueeze(1) for feat in self.y_vars], dim=1).float()\n",
    "            else:\n",
    "                y = torch.zeros((1))\n",
    "            if use_gpu:\n",
    "                yield (x.cuda(), y.cuda())\n",
    "            else:\n",
    "                yield (x, y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Batch Wrapper class accepts the iterator variable itself, variable x name, and variable y name during initialisation. It _yields_ tensor x and y. x and y values are looked up from the `batch` in `self.dl` using `getattr`.\n",
    "\n",
    "If GPU is available, this class moves these tensors to the GPU as well with `x.cuda()` and `y.cuda()` making it ready for consumption by the model. \n",
    "\n",
    "Let's quickly wrap our train, val and test iter object using this new class: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = BatchWrapper(train_iter, \"comment_text\", [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"])\n",
    "valid_dl = BatchWrapper(val_iter, \"comment_text\", [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"])\n",
    "test_dl = BatchWrapper(test_iter, \"comment_text\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns the simplest iterator, ready for model processing. Note that in this particular case, the tensor has a 'device' attribute set to 'cuda:0'. Let's preview this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 453,   63,   15,  ...,  454,  660,  778],\n",
       "         [ 523,    4,  601,  ...,   78,   11,  650],\n",
       "         [  30,  664,  242,  ...,    8,    2,   22],\n",
       "         ...,\n",
       "         [   1,    1,    1,  ...,    1,    1,    1],\n",
       "         [   1,    1,    1,  ...,    1,    1,    1],\n",
       "         [   1,    1,    1,  ...,    1,    1,    1]], device='cuda:0'),\n",
       " tensor([[ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 1.,  1.,  0.,  1.,  1.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.]], device='cuda:0'))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_dl.__iter__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Text Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready for training our text classifier model. Let's start with something simple, we are going to consider this model to be a black box for now. \n",
    "\n",
    "Model architecture is better explained by several Youtube videos such as those by cs224n.github.io . I suggest that you explore and connect it with the know-how that you already have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLSTMBaseline(nn.Module):\n",
    "    def __init__(self, hidden_dim, emb_dim=300,\n",
    "                 spatial_dropout=0.05, recurrent_dropout=0.1, num_linear=2):\n",
    "        super().__init__() # don't forget to call this!\n",
    "        self.embedding = nn.Embedding(len(TEXT.vocab), emb_dim)\n",
    "        self.encoder = nn.LSTM(emb_dim, hidden_dim, num_layers=num_linear, dropout=recurrent_dropout)\n",
    "        self.linear_layers = []\n",
    "        for _ in range(num_linear - 1):\n",
    "            self.linear_layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "        self.linear_layers = nn.ModuleList(self.linear_layers)\n",
    "        self.predictor = nn.Linear(hidden_dim, 6)\n",
    "    \n",
    "    def forward(self, seq):\n",
    "        hdn, _ = self.encoder(self.embedding(seq))\n",
    "        feature = hdn[-1, :, :]\n",
    "        for layer in self.linear_layers:\n",
    "            feature = layer(feature)\n",
    "        preds = self.predictor(feature)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the Model\n",
    "\n",
    "Any Pytorch model is instantiated like a Python object. Unlike Tensorflow, there is no strict notion of a session object inside which the code is compiled and then run. The model class is as we have written above. \n",
    "\n",
    "The init function of the class above accepts a few parameters: \n",
    "- hidden_dim: hidden layer dimensions - is the vector length of the hidden layers\n",
    "- emb_dim=300: embedding dimension - is the vector length of the first input _step_ to the LSTM\n",
    "- num_linear=2\n",
    "The other two dropout parameters: \n",
    "- spatial_dropout=0.05, \n",
    "- recurrent_dropout=0.1, \n",
    "\n",
    "Both act as regularizers. They help prevent the model from overfitting or the state where the model ends up learning the samples in training set instead of the more generic pattern which can be used to make predictions. \n",
    "\n",
    "One way to think about the differences between the dropouts is that one of them acts on the input itself. The other acts during backpropagation or weight update step as mentioned learner.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleLSTMBaseline(\n",
      "  (embedding): Embedding(784, 300)\n",
      "  (encoder): LSTM(300, 500, num_layers=2, dropout=0.1)\n",
      "  (linear_layers): ModuleList(\n",
      "    (0): Linear(in_features=500, out_features=500, bias=True)\n",
      "  )\n",
      "  (predictor): Linear(in_features=500, out_features=6, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "em_sz = 300\n",
    "nh = 500\n",
    "model = SimpleLSTMBaseline(nh, emb_dim=em_sz)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a small utility function to calculate the size of any PyTorch model. By size, we mean the number of model parameters which can be updated during training to learn the input-to-target mapping. \n",
    "\n",
    "While this function is pre-implemented in Keras, it's simple enough to write it again for us: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.096706 million parameters\n"
     ]
    }
   ],
   "source": [
    "def model_size(model: torch.nn)->int:\n",
    "    \"\"\"\n",
    "    Calculates the number of trainable parameters in any model\n",
    "    \n",
    "    Returns:\n",
    "        params (int): the total count of all model weights\n",
    "    \"\"\"\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "#     model_parameters = model.parameters()\n",
    "    params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "    return params\n",
    "\n",
    "print(f'{model_size(model)/10**6} million parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the model has little more than 4 million parameters, in comparison, a typical decision tree might only have a few hundred decision splits at maximum.\n",
    "\n",
    "Next, we move the model weigths to GPU using the familiar `.cuda()` syntax. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_gpu:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Putting together the pieces again**:\n",
    "\n",
    "- Loss Function: Binary Cross Entropy with Logit Loss. It serves as the quality metric of how far are the predictions from the ground truth. \n",
    "- Optimizer: We use the Adam optimizer with default parameters, set with learning rate of 1e-2 or 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "opt = optim.Adam(model.parameters(), lr=1e-2)\n",
    "loss_func = nn.BCEWithLogitsLoss().cuda()\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop is logically split into sections. Notice the placement of following lines of code: \n",
    "- `model.train()`\n",
    "- `model.eval()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 13.5037, Validation Loss: 4.6498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss: 7.8243, Validation Loss: 24.5401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss: 57.4577, Validation Loss: 4.0107\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for epoch in range(1, epochs + 1):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    model.train() # turn on training mode\n",
    "    for x, y in tqdm(train_dl): # thanks to our wrapper, we can intuitively iterate over our data!\n",
    "        opt.zero_grad()\n",
    "        preds = model(x)\n",
    "        loss = loss_func(preds, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        running_loss += loss.item() * x.size(0)\n",
    "        \n",
    "    epoch_loss = running_loss / len(trn)\n",
    "    \n",
    "    # calculate the validation loss for this epoch\n",
    "    val_loss = 0.0\n",
    "    model.eval() # turn on evaluation mode\n",
    "    for x, y in valid_dl:\n",
    "        preds = model(x)\n",
    "        loss = loss_func(preds, y)\n",
    "        val_loss += loss.item() * x.size(0)\n",
    "\n",
    "    val_loss /= len(vld)\n",
    "    print('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}'.format(epoch, epoch_loss, val_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  9.64it/s]\n"
     ]
    }
   ],
   "source": [
    "test_preds = []\n",
    "for x, y in tqdm(test_dl):\n",
    "    preds = model(x)\n",
    "    # if you're data is on the GPU, you need to move the data back to the cpu\n",
    "    # preds = preds.data.cpu().numpy()\n",
    "    preds = preds.data.cpu().numpy()\n",
    "    # the actual outputs of the model are logits, so we need to pass these values to the sigmoid function\n",
    "    preds = 1 / (1 + np.exp(-preds))\n",
    "    test_preds.append(preds)\n",
    "test_preds = np.hstack(test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert predictions to a pandas dataframe\n",
    "\n",
    "This helps us convert the predictions to a more interpretable format. Let's insert the predictions in the correct column and then we can preview few rows of the dataframe: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"data/test.csv\")\n",
    "for i, col in enumerate([\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]):\n",
    "    test_df[col] = test_preds[:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
       "      <td>0.629146</td>\n",
       "      <td>0.116721</td>\n",
       "      <td>0.438606</td>\n",
       "      <td>0.156848</td>\n",
       "      <td>0.139696</td>\n",
       "      <td>0.388736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>== From RfC == \\r\\n\\r\\n The title is fine as i...</td>\n",
       "      <td>0.629146</td>\n",
       "      <td>0.116721</td>\n",
       "      <td>0.438606</td>\n",
       "      <td>0.156848</td>\n",
       "      <td>0.139696</td>\n",
       "      <td>0.388736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>\" \\r\\n\\r\\n == Sources == \\r\\n\\r\\n * Zawe Ashto...</td>\n",
       "      <td>0.629146</td>\n",
       "      <td>0.116721</td>\n",
       "      <td>0.438606</td>\n",
       "      <td>0.156848</td>\n",
       "      <td>0.139696</td>\n",
       "      <td>0.388736</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  \\\n",
       "0  00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...   \n",
       "1  0000247867823ef7  == From RfC == \\r\\n\\r\\n The title is fine as i...   \n",
       "2  00013b17ad220c46  \" \\r\\n\\r\\n == Sources == \\r\\n\\r\\n * Zawe Ashto...   \n",
       "\n",
       "      toxic  severe_toxic   obscene    threat    insult  identity_hate  \n",
       "0  0.629146      0.116721  0.438606  0.156848  0.139696       0.388736  \n",
       "1  0.629146      0.116721  0.438606  0.156848  0.139696       0.388736  \n",
       "2  0.629146      0.116721  0.438606  0.156848  0.139696       0.388736  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
