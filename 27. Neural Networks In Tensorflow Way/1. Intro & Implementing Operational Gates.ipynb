{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks are currently breaking records in tasks such as image and speech recognition, reading handwriting, understanding text, image segmentation, dialogue systems, autonomous car driving, and so much more.\n",
    "\n",
    "The concept of a neural network has been around for decades. However, it only recently gained traction because we now have the computational power to train large networks because of advances in processing power, algorithm efficiency, and data sizes.\n",
    "\n",
    "A neural network is basically a sequence of operations applied to a matrix of input data. These operations are usually collections of additions and multiplications followed by applications of non-linear functions.\n",
    "\n",
    "The important trick with neural networks is called 'back propagation'. Back propagation is a procedure that allows us to update the model variables based on the learning rate and the output of the loss function. We used back propagation to update our model variables in our previous programs\n",
    "\n",
    "Another important feature to take note of in neural networks is the non-linear activation function. Since most neural networks are just combinations of addition and multiplication operations, they will not be able to model non-linear data sets. To address this issue, we have used the non-linear activation functions in the neural networks. This will allow the neural network to adapt to most non-linear situations.\n",
    "\n",
    "It is important to remember that, like most of the algorithms we have seen so far, neural networks are sensitive to the hyper-parameters that we choose. Today, we will see the impact of different learning rates, loss functions, and optimization procedures.\n",
    "\n",
    "There are more resources for learning about neural networks that are more in depth and detailed. Here are some following resources:\n",
    "\n",
    "* The seminal paper describing back propagation is Efficient Back Prop by Yann LeCun et. al. The PDF is located here: http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf\n",
    "\n",
    "* CS231, Convolutional Neural Networks for Visual Recognition, by Stanford University, class resources available here: http://cs231n.stanford.edu/\n",
    "\n",
    "* CS224d, Deep Learning for Natural Language Processing, by Stanford University, class resources available here: http://cs224d.stanford.edu/\n",
    "\n",
    "* Deep Learning, a book by the MIT Press. Goodfellow, et. al. 2016. Located: http://www.deeplearningbook.org\n",
    "\n",
    "* There is an online book called Neural Networks and Deep Learning by Michael Nielsen, located here: http://neuralnetworksanddeeplearning.com/\n",
    "\n",
    "* For a more pragmatic approach and introduction to neural networks, Andrej Karpathy has written a great summary and JavaScript examples called A Hacker's Guide to Neural Networks. The write up is located here: http://karpathy.github.io/neuralnets/\n",
    "\n",
    "* Another site that summarizes some good notes on deep learning is called Deep Learning for Beginners by Ian Goodfellow, Yoshua Bengio, and Aaron Courville. This web page can be found here: http://randomekek.github.io/deep/deeplearning.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing an Operational Gate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import h5py\n",
    "warnings.resetwarnings()\n",
    "\n",
    "import tensorflow as tf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***Gate 1***\n",
    "\n",
    "Create a multiplication gate:  \n",
    "\n",
    "$f(x) = a * x$\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "  a---\n",
    "      |\n",
    "      |---- (multiply) --> output\n",
    "      |\n",
    "  x --\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing a Multiplication Gate Output to 50.\n",
      "7.0 * 5.0 = 35.0\n",
      "8.5 * 5.0 = 42.5\n",
      "9.25 * 5.0 = 46.25\n",
      "9.625 * 5.0 = 48.125\n",
      "9.8125 * 5.0 = 49.0625\n",
      "9.90625 * 5.0 = 49.53125\n",
      "9.953125 * 5.0 = 49.765625\n",
      "9.9765625 * 5.0 = 49.882812\n",
      "9.988281 * 5.0 = 49.941406\n",
      "9.994141 * 5.0 = 49.970703\n"
     ]
    }
   ],
   "source": [
    "# Start Graph Session\n",
    "sess = tf.Session()\n",
    "\n",
    "a = tf.Variable(tf.constant(4.))\n",
    "x_val = 5.\n",
    "x_data = tf.placeholder(dtype=tf.float32)\n",
    "\n",
    "multiplication = tf.multiply(a, x_data)\n",
    "\n",
    "# Declare the loss function as the difference between\n",
    "# the output and a target value, 50.\n",
    "loss = tf.square(tf.subtract(multiplication, 50.))\n",
    "\n",
    "# Initialize variables\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "# Declare optimizer\n",
    "my_opt = tf.train.GradientDescentOptimizer(0.01)\n",
    "train_step = my_opt.minimize(loss)\n",
    "\n",
    "# Run loop across gate\n",
    "print('Optimizing a Multiplication Gate Output to 50.')\n",
    "for i in range(10):\n",
    "    sess.run(train_step, feed_dict={x_data: x_val})\n",
    "    a_val = sess.run(a)\n",
    "    mult_output = sess.run(multiplication, feed_dict={x_data: x_val})\n",
    "    print(str(a_val) + ' * ' + str(x_val) + ' = ' + str(mult_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Gate 2***\n",
    "Create a nested gate: $f(x) = a * x + b$"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "a --\n",
    "      |\n",
    "      |-- (multiply)--\n",
    "      |               |\n",
    "  x --                |-- (add) --> output\n",
    "                      |\n",
    "                  b --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimizing Two Gate Output to 50.\n",
      "5.4 * 5.0 + 1.88 = 28.88\n",
      "7.512 * 5.0 + 2.3024 = 39.8624\n",
      "8.52576 * 5.0 + 2.5051522 = 45.133953\n",
      "9.012364 * 5.0 + 2.6024733 = 47.664295\n",
      "9.2459345 * 5.0 + 2.6491873 = 48.87886\n",
      "9.358048 * 5.0 + 2.67161 = 49.461853\n",
      "9.411863 * 5.0 + 2.682373 = 49.74169\n",
      "9.437695 * 5.0 + 2.687539 = 49.87601\n",
      "9.450093 * 5.0 + 2.690019 = 49.940483\n",
      "9.456045 * 5.0 + 2.6912093 = 49.971436\n"
     ]
    }
   ],
   "source": [
    "# Start a New Graph Session\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "a = tf.Variable(tf.constant(1.))\n",
    "b = tf.Variable(tf.constant(1.))\n",
    "x_val = 5.\n",
    "x_data = tf.placeholder(dtype=tf.float32)\n",
    "\n",
    "two_gate = tf.add(tf.multiply(a, x_data), b)\n",
    "\n",
    "# Declare the loss function as the difference between\n",
    "# the output and a target value, 50.\n",
    "loss = tf.square(tf.subtract(two_gate, 50.))\n",
    "\n",
    "# Initialize variables\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "# Declare optimizer\n",
    "my_opt = tf.train.GradientDescentOptimizer(0.01)\n",
    "train_step = my_opt.minimize(loss)\n",
    "\n",
    "# Run loop across gate\n",
    "print('\\nOptimizing Two Gate Output to 50.')\n",
    "for i in range(10):\n",
    "    sess.run(train_step, feed_dict={x_data: x_val})\n",
    "    a_val, b_val = (sess.run(a), sess.run(b))\n",
    "    two_gate_output = sess.run(two_gate, feed_dict={x_data: x_val})\n",
    "    print(str(a_val) + ' * ' + str(x_val) + ' + ' + str(b_val) + ' = ' + str(two_gate_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
